{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import  matplotlib.pyplot as plt\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "from datetime import datetime\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(\"../../SongApp.csv\",encoding ='utf_8',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../../SongTst.csv\",encoding ='utf_8',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>49.94357</th>\n",
       "      <th>21.47114</th>\n",
       "      <th>73.07750</th>\n",
       "      <th>8.74861</th>\n",
       "      <th>-17.40628</th>\n",
       "      <th>-13.09905</th>\n",
       "      <th>-25.01202</th>\n",
       "      <th>-12.23257</th>\n",
       "      <th>7.83089</th>\n",
       "      <th>...</th>\n",
       "      <th>13.01620</th>\n",
       "      <th>-54.40548</th>\n",
       "      <th>58.99367</th>\n",
       "      <th>15.37344</th>\n",
       "      <th>1.11144</th>\n",
       "      <th>-23.08793</th>\n",
       "      <th>68.40795</th>\n",
       "      <th>-1.82223</th>\n",
       "      <th>-27.46348</th>\n",
       "      <th>2.26327</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>463715</td>\n",
       "      <td>52.67814</td>\n",
       "      <td>-2.88914</td>\n",
       "      <td>43.95268</td>\n",
       "      <td>-1.39209</td>\n",
       "      <td>-14.93379</td>\n",
       "      <td>-15.86877</td>\n",
       "      <td>1.19379</td>\n",
       "      <td>0.31401</td>\n",
       "      <td>-4.44235</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.74356</td>\n",
       "      <td>-42.57910</td>\n",
       "      <td>-2.91103</td>\n",
       "      <td>48.72805</td>\n",
       "      <td>-3.08183</td>\n",
       "      <td>-9.38888</td>\n",
       "      <td>-7.27179</td>\n",
       "      <td>-4.00966</td>\n",
       "      <td>-68.96211</td>\n",
       "      <td>-5.21525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>463716</td>\n",
       "      <td>45.74235</td>\n",
       "      <td>12.02291</td>\n",
       "      <td>11.03009</td>\n",
       "      <td>-11.60763</td>\n",
       "      <td>11.80054</td>\n",
       "      <td>-11.12389</td>\n",
       "      <td>-5.39058</td>\n",
       "      <td>-1.11981</td>\n",
       "      <td>-7.74086</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.70606</td>\n",
       "      <td>-24.22599</td>\n",
       "      <td>-35.22686</td>\n",
       "      <td>27.77729</td>\n",
       "      <td>15.38934</td>\n",
       "      <td>58.20036</td>\n",
       "      <td>-61.12698</td>\n",
       "      <td>-10.92522</td>\n",
       "      <td>26.75348</td>\n",
       "      <td>-5.78743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>463717</td>\n",
       "      <td>52.55883</td>\n",
       "      <td>2.87222</td>\n",
       "      <td>27.38848</td>\n",
       "      <td>-5.76235</td>\n",
       "      <td>-15.35766</td>\n",
       "      <td>-15.01592</td>\n",
       "      <td>-5.86893</td>\n",
       "      <td>-0.31447</td>\n",
       "      <td>-5.06922</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.35215</td>\n",
       "      <td>-16.86791</td>\n",
       "      <td>-10.58277</td>\n",
       "      <td>40.10173</td>\n",
       "      <td>-0.54005</td>\n",
       "      <td>-11.54746</td>\n",
       "      <td>-45.35860</td>\n",
       "      <td>-4.55694</td>\n",
       "      <td>-43.17368</td>\n",
       "      <td>-3.33725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>463718</td>\n",
       "      <td>51.34809</td>\n",
       "      <td>9.02702</td>\n",
       "      <td>25.33757</td>\n",
       "      <td>-6.62537</td>\n",
       "      <td>0.03367</td>\n",
       "      <td>-12.69565</td>\n",
       "      <td>-3.13400</td>\n",
       "      <td>2.98649</td>\n",
       "      <td>-6.71750</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.87366</td>\n",
       "      <td>-20.03371</td>\n",
       "      <td>-66.38940</td>\n",
       "      <td>50.56569</td>\n",
       "      <td>0.27747</td>\n",
       "      <td>67.05657</td>\n",
       "      <td>-55.58846</td>\n",
       "      <td>-7.50859</td>\n",
       "      <td>28.23511</td>\n",
       "      <td>-0.72045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>463719</td>\n",
       "      <td>45.84640</td>\n",
       "      <td>2.83376</td>\n",
       "      <td>-6.00506</td>\n",
       "      <td>-15.16150</td>\n",
       "      <td>-10.72385</td>\n",
       "      <td>-15.15233</td>\n",
       "      <td>5.00724</td>\n",
       "      <td>1.69039</td>\n",
       "      <td>-0.95527</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.97255</td>\n",
       "      <td>-76.79480</td>\n",
       "      <td>55.54104</td>\n",
       "      <td>88.86441</td>\n",
       "      <td>-8.43241</td>\n",
       "      <td>62.00507</td>\n",
       "      <td>123.56146</td>\n",
       "      <td>7.87100</td>\n",
       "      <td>-38.61680</td>\n",
       "      <td>26.41166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  49.94357  21.47114  73.07750   8.74861  -17.40628  -13.09905  \\\n",
       "0      463715  52.67814  -2.88914  43.95268  -1.39209  -14.93379  -15.86877   \n",
       "1      463716  45.74235  12.02291  11.03009 -11.60763   11.80054  -11.12389   \n",
       "2      463717  52.55883   2.87222  27.38848  -5.76235  -15.35766  -15.01592   \n",
       "3      463718  51.34809   9.02702  25.33757  -6.62537    0.03367  -12.69565   \n",
       "4      463719  45.84640   2.83376  -6.00506 -15.16150  -10.72385  -15.15233   \n",
       "\n",
       "   -25.01202  -12.23257  7.83089    ...     13.01620  -54.40548  58.99367  \\\n",
       "0    1.19379    0.31401 -4.44235    ...     -5.74356  -42.57910  -2.91103   \n",
       "1   -5.39058   -1.11981 -7.74086    ...     -4.70606  -24.22599 -35.22686   \n",
       "2   -5.86893   -0.31447 -5.06922    ...     -8.35215  -16.86791 -10.58277   \n",
       "3   -3.13400    2.98649 -6.71750    ...     -6.87366  -20.03371 -66.38940   \n",
       "4    5.00724    1.69039 -0.95527    ...    -27.97255  -76.79480  55.54104   \n",
       "\n",
       "   15.37344   1.11144  -23.08793   68.40795  -1.82223  -27.46348   2.26327  \n",
       "0  48.72805  -3.08183   -9.38888   -7.27179  -4.00966  -68.96211  -5.21525  \n",
       "1  27.77729  15.38934   58.20036  -61.12698 -10.92522   26.75348  -5.78743  \n",
       "2  40.10173  -0.54005  -11.54746  -45.35860  -4.55694  -43.17368  -3.33725  \n",
       "3  50.56569   0.27747   67.05657  -55.58846  -7.50859   28.23511  -0.72045  \n",
       "4  88.86441  -8.43241   62.00507  123.56146   7.87100  -38.61680  26.41166  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = all_data.drop(['Unnamed: 0'],axis = 1, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = all_data.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_l = []\n",
    "for i in range(100000):\n",
    "    indice = random.randrange(0,len(mat))\n",
    "    ligne = mat[indice]\n",
    "    new_data_l.append(ligne)\n",
    "new_data = pd.DataFrame(new_data_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_labels(labels):\n",
    "    new_label = []\n",
    "    mini = min(labels)\n",
    "    maxi = max(labels)\n",
    "    for i in range(len(labels)):\n",
    "        new_label.append((labels[i]-mini)/(maxi-mini))\n",
    "    return new_label,maxi,mini\n",
    "\n",
    "def normalise_data(data):\n",
    "    maxi = np.array(data).max()\n",
    "    mini = np.array(data).min()\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        ligne = []\n",
    "        for j in range(len(data[i])):\n",
    "            ligne.append((data[i][j]-mini)/(maxi-mini))\n",
    "        new_data.append(ligne)\n",
    "    return new_data\n",
    "\n",
    "def taux_erreur(modele ,data,label,loss):\n",
    "    erreur = 0\n",
    "    for i in range(len(data)):\n",
    "        x = torch.FloatTensor(data[i])\n",
    "        x = Variable(x.view(-1,taille_vect),requires_grad=True)\n",
    "        y = torch.FloatTensor(1)\n",
    "        y.zero_()\n",
    "        y[0] = (label[i])\n",
    "        y = Variable(y,requires_grad=False)\n",
    "        y_pred = modele(x)\n",
    "        err  = loss(y_pred , y)\n",
    "        erreur+= err.data[0]\n",
    "    return erreur/(len(data)*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels,maxi,mini = normalise_labels(new_data[0].as_matrix())\n",
    "data = normalise_data(new_data.drop([0],axis = 1, inplace = False).as_matrix())\n",
    "taille_vect = len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear_Model(torch.nn.Module):\n",
    "    def __init__(self , X ,Y) :\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.modele = torch.nn.Linear(X,Y)    \n",
    "        \n",
    "    def forward(self , x):\n",
    "        return (torch.tanh(self.modele(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Out_Model(torch.nn.Module):\n",
    "    def __init__(self , X ,Y) :\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.modele = torch.nn.Linear(X,Y)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        return (torch.sigmoid(self.modele(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(modele , loss,data,nb_iter,step,label):\n",
    "    tab_err_train = []\n",
    "    i = 0\n",
    "    data_train = []\n",
    "    label_train = []\n",
    "    while i < nb_iter :\n",
    "        indice = random.randint(0,len(data)-1)\n",
    "        x = torch.FloatTensor(data[indice])\n",
    "        x = Variable(x.view(-1,taille_vect))\n",
    "        y = torch.FloatTensor(final_out)\n",
    "        y[0] = label[indice]\n",
    "        y = Variable(y)\n",
    "        y_pred = modele(x)\n",
    "        erreur  = loss(y_pred , y)\n",
    "        modele.zero_grad()\n",
    "        erreur.backward()\n",
    "        for param in modele.parameters():\n",
    "            param.data -= step * param.grad.data\n",
    "        i+=1\n",
    "        if(i % 2 ==0):\n",
    "            err = taux_erreur(modele ,data,label,loss)\n",
    "            print ('    Erreur en train : ',err)\n",
    "            print(' iter : ',i)\n",
    "    return tab_err_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_out = 1\n",
    "output = 1500#nb neuronne couche cachÃ©\n",
    "models = {}\n",
    "loss = torch.nn.MSELoss()\n",
    "modele = torch.nn.Sequential(Linear_Model(taille_vect,output),Linear_Model(output,output),Out_Model(output,final_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Erreur en train :  0.0942472671997116\n",
      " iter :  2\n",
      "    Erreur en train :  0.08169375891850747\n",
      " iter :  4\n",
      "    Erreur en train :  0.07408162646634962\n",
      " iter :  6\n",
      "    Erreur en train :  0.06860907850604253\n",
      " iter :  8\n",
      "    Erreur en train :  0.06105726836371302\n",
      " iter :  10\n",
      "    Erreur en train :  0.05458048406869307\n",
      " iter :  12\n",
      "    Erreur en train :  0.04802147540257266\n",
      " iter :  14\n",
      "    Erreur en train :  0.04676839221942532\n",
      " iter :  16\n",
      "    Erreur en train :  0.04190118255902229\n",
      " iter :  18\n",
      "    Erreur en train :  0.04195687640671769\n",
      " iter :  20\n",
      "    Erreur en train :  0.03823261424175677\n",
      " iter :  22\n",
      "    Erreur en train :  0.0350900757313191\n",
      " iter :  24\n",
      "    Erreur en train :  0.031727628110015255\n",
      " iter :  26\n",
      "    Erreur en train :  0.029121084906712367\n",
      " iter :  28\n",
      "    Erreur en train :  0.026534175116767363\n",
      " iter :  30\n",
      "    Erreur en train :  0.02474351719382753\n",
      " iter :  32\n",
      "    Erreur en train :  0.024676266145280606\n",
      " iter :  34\n",
      "    Erreur en train :  0.02423297156937698\n",
      " iter :  36\n",
      "    Erreur en train :  0.022784401511081565\n",
      " iter :  38\n",
      "    Erreur en train :  0.02442145782527679\n",
      " iter :  40\n",
      "    Erreur en train :  0.023895052895571416\n",
      " iter :  42\n",
      "    Erreur en train :  0.023450382660724963\n",
      " iter :  44\n",
      "    Erreur en train :  0.02233043203060791\n",
      " iter :  46\n",
      "    Erreur en train :  0.021602391141199783\n",
      " iter :  48\n",
      "    Erreur en train :  0.021820064496691757\n",
      " iter :  50\n",
      "    Erreur en train :  0.02076087718099643\n",
      " iter :  52\n",
      "    Erreur en train :  0.020050226024393228\n",
      " iter :  54\n",
      "    Erreur en train :  0.01922089644726846\n",
      " iter :  56\n",
      "    Erreur en train :  0.018525886196009977\n",
      " iter :  58\n",
      "    Erreur en train :  0.018092936099727713\n",
      " iter :  60\n",
      "    Erreur en train :  0.017792100666759594\n",
      " iter :  62\n",
      "    Erreur en train :  0.01855631268476817\n",
      " iter :  64\n",
      "    Erreur en train :  0.01813387839199859\n",
      " iter :  66\n",
      "    Erreur en train :  0.017657475386439928\n",
      " iter :  68\n",
      "    Erreur en train :  0.018281253882317643\n",
      " iter :  70\n",
      "    Erreur en train :  0.01869843064890712\n",
      " iter :  72\n",
      "    Erreur en train :  0.01828632822423539\n",
      " iter :  74\n",
      "    Erreur en train :  0.017966200068882575\n",
      " iter :  76\n",
      "    Erreur en train :  0.018409969964488668\n",
      " iter :  78\n",
      "    Erreur en train :  0.018070854609079045\n",
      " iter :  80\n",
      "    Erreur en train :  0.018039675197826562\n",
      " iter :  82\n",
      "    Erreur en train :  0.01772376598416045\n",
      " iter :  84\n",
      "    Erreur en train :  0.017279390531950677\n",
      " iter :  86\n",
      "    Erreur en train :  0.016857116580388572\n",
      " iter :  88\n",
      "    Erreur en train :  0.01678099604093888\n",
      " iter :  90\n"
     ]
    }
   ],
   "source": [
    "step = 0.01\n",
    "nb_iter =100\n",
    "tab_err_train = train(modele,loss,data,nb_iter,step,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(data,maxi,mini,label):\n",
    "    x = torch.FloatTensor(data)\n",
    "    x = Variable(x.view(-1,taille_vect))\n",
    "    y_pred = modele(x)\n",
    "    y_denormalise = round((y_pred*(maxi-mini)+mini).data[0][0])\n",
    "    label_denormalise = (label*(maxi-mini)+mini)\n",
    "    print (\"Valeure predite : \",(y_denormalise),\" Valeure attendue : \",label_denormalise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    inference(data[i],maxi,mini,labels[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
